package org.sparkexample;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.MapWritable;
import org.apache.hadoop.io.Text;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.elasticsearch.hadoop.mr.EsInputFormat;

import scala.Tuple2;


public class ESInputSpark {

	public static void main(String[] argv) {
		System.setProperty("hadoop.home.dir", "/usr/local/hadoop");
		SparkConf conf = new SparkConf().setAppName("WordCount").setMaster("local");

		conf.set("spark.serializer", org.apache.spark.serializer.KryoSerializer.class.getName());

		JavaSparkContext sc = new JavaSparkContext(conf);

		Configuration hadoopConfiguration = new Configuration();
		hadoopConfiguration.set("es.nodes", "localhost:9200");  // ????
		hadoopConfiguration.set("es.resource", "megacorp/employee");  // ????
		
//		hadoopConfiguration.set("es.nodes", "http://10.220.83.22:9206");  // ????
//		hadoopConfiguration.set("es.resource", "products/product");  // ????

		@SuppressWarnings("unchecked")
		JavaPairRDD<Text, MapWritable> esRDD = sc.newAPIHadoopRDD(hadoopConfiguration, EsInputFormat.class, Text.class,
				MapWritable.class);
		System.out.println("Count of records founds is " + esRDD.count());

		// This function will get ES record key as first parameter and the ES
		// record as second parameter, it will return {city,1} tuple for each
		// city in the record
		JavaPairRDD<String, Integer> firstNameCountMap = esRDD
				.mapToPair(new PairFunction<Tuple2<Text, MapWritable>, String, Integer>() {
					public Tuple2<String, Integer> call(Tuple2<Text, MapWritable> currentEntry) throws Exception {
						MapWritable valueMap = currentEntry._2();
						Text first_name = (Text) valueMap.get(new Text("first_name"));
						System.out.printf("Name %s\n", first_name.toString());
						return new Tuple2<String, Integer>(first_name.toString(), 1);
					}
				});

		// This is reducer which will maintain running count of city vs count
		JavaPairRDD<String, Integer> firstNameCount = firstNameCountMap.reduceByKey(new Function2<Integer, Integer, Integer>() {
			public Integer call(Integer first, Integer second) throws Exception {
				return first + second;
			}
		});
		
		JavaPairRDD<String, Integer> firstNameCount2 = firstNameCount.repartition(1);

		firstNameCount2.saveAsTextFile(argv[0]); //"/home/duc07/Programs/spark/results"

	}
}